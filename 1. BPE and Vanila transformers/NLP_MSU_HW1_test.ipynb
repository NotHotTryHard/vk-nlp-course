{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBA5IPyb4BFH"
      },
      "source": [
        "# –ï–≥–æ –≤–µ–ª–∏—á–µ—Å—Ç–≤–æ, \"–¥–æ–º–∞—à–∫–∞ ‚Ññ1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOnY6pwvpghE"
      },
      "source": [
        "–í —ç—Ç–æ–π –¥–æ–º–∞—à–Ω–µ–π —Ä–∞–±–æ—Ç–µ –≤–∞–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—Å—è —É–Ω–∏–∫–∞–ª—å–Ω–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±—É—á–∏—Ç—å Byte-level BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –Ω–µ–±–æ–ª—å—à—É—é LM.  \n",
        "\n",
        "–î–æ–º–∞—à–Ω—è—è —Ä–∞–±–æ—Ç–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤: —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Transformer –º–æ–¥–µ–ª–∏ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ —Å —Ä—É—Å—Å–∫–∏–º–∏ –∞–Ω–µ–∫–¥–æ—Ç–∞–º–∏!\n",
        "\n",
        "–û–±—É—á–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å –º–æ–∂–Ω–æ –∏ –Ω—É–∂–Ω–æ –≤—ã–ª–æ–∂–∏—Ç—å –Ω–∞ [ü§ó HuggingFace](https://huggingface.co/). –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å —Ç–∞–º, –ø–æ–¥–ø–∏—à–∏—Ç–µ—Å—å –Ω–∞ [deep vk](https://huggingface.co/deepvk) –∏ —Å–æ–∑–¥–∞–π—Ç–µ —Å–µ–±–µ API —Ç–æ–∫–µ–Ω.\n",
        "\n",
        "–°–ª–µ–¥—É–π—Ç–µ —è—á–µ–π–∫–∞–º —Ç–µ—Ç—Ä–∞–¥–∫–∏ –∏ –∑–∞–ø–æ–ª–Ω—è–π—Ç–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —è—á–µ–π–∫–∏. –í –∫–æ–Ω—Ü–µ —Ç–µ—Ç—Ä–∞–¥–∫–∏ –≤—ã –Ω–∞–π–¥–µ—Ç–µ –∑–∞–¥–∞—á–∏ —Å–æ –∑–≤–µ–∑–¥–æ—á–∫–æ–π, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0byNYx5dzB4b",
        "outputId": "4f5ebf72-8cae-4856-dfc6-ae01271ddc62"
      },
      "outputs": [],
      "source": [
        "# –£—Å—Ç–∞–Ω–æ–≤–∏–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "\n",
        "#%pip install --quiet datasets livelossplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UILR1tu3z9oI"
      },
      "outputs": [],
      "source": [
        "# –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–º–ø–æ—Ä—Ç—ã\n",
        "\n",
        "import inspect\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from functools import lru_cache, partial\n",
        "from pathlib import Path\n",
        "\n",
        "import regex as re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfApi, PyTorchModelHubMixin, interpreter_login, snapshot_download\n",
        "from huggingface_hub.utils import SoftTemporaryDirectory\n",
        "from livelossplot import PlotLosses\n",
        "from torch import Tensor\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm, trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p7OLSivnPW0"
      },
      "outputs": [],
      "source": [
        "# –≠—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –±—É–¥—É—Ç –ø–æ–º–µ—á–µ–Ω—ã –≤—Å–µ –º–µ—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–∑–∞–ø–æ–ª–Ω–∏—Ç—å\n",
        "# –≠—Ç–æ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–∞–∫ —Ü–µ–ª—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, —Ç–∞–∫ –∏ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–∏ –≤–Ω—É—Ç—Ä–∏ –Ω–∏—Ö\n",
        "# –í—Å–µ–≥–¥–∞ –º–æ–∂–Ω–æ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∏–Ω—Ç—Ä–æ—Å–ø–µ–∫—Ü–∏–µ–π –∏ –Ω–∞–π—Ç–∏ –º–µ—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ :)\n",
        "\n",
        "\n",
        "def todo():\n",
        "    stack = inspect.stack()\n",
        "    caller_frame = stack[1]\n",
        "    function_name = caller_frame.function\n",
        "    line_number = caller_frame.lineno\n",
        "    raise NotImplementedError(f\"TODO at {function_name}, line {line_number}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCkJJp2JK99x",
        "outputId": "3725be63-2cc3-471a-ad16-127334b85556"
      },
      "outputs": [],
      "source": [
        "interpreter_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVWKkwaryDTq",
        "outputId": "e15b8269-e8ce-4bac-bd3a-cb395c8f4761"
      },
      "outputs": [],
      "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –¥–ª—è –±—É–¥—É—â–µ–π –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
        "username = HfApi().whoami()[\"name\"]\n",
        "REPO_NAME = f\"{username}/llm-course-hw1\"  # –ò–ª–∏ –∫–∞–∫ –≤–∞–º —Ö–æ—á–µ—Ç—Å—è\n",
        "\n",
        "print(f\"Homework repository: '{REPO_NAME}'\")\n",
        "\n",
        "# –ò –¥—Ä—É–≥–∏–µ –ø–æ–ª–µ–∑–Ω—ã–µ –≤–µ—â–∏\n",
        "SEED = 0xC0FFEE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxN5JUbZ3ToV"
      },
      "source": [
        "# –î–∞—Ç–∞—Å–µ—Ç\n",
        "\n",
        "–ü–µ—Ä–≤—ã–º –¥–µ–ª–æ–º –∑–∞–≥—Ä—É–∑–∏–º –¥–∞–Ω–Ω—ã–µ: [ü§ó IgorVolochay/russian_jokes](https://huggingface.co/datasets/IgorVolochay/russian_jokes)\n",
        "\n",
        "–ò –Ω–µ–º–Ω–æ–≥–æ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –Ω–∏—Ö üëÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from datasets import load_dataset, load_from_disk\n",
        "\n",
        "dataset_dir = \"data\"\n",
        "\n",
        "if os.path.exists(dataset_dir):\n",
        "    dataset = load_from_disk(dataset_dir)\n",
        "else:\n",
        "    dataset = load_dataset(\"IgorVolochay/russian_jokes\", data_files=\"dataset.csv\")\n",
        "    dataset.save_to_disk(dataset_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "a7cfb7fe4e0e484784c16947848f296e",
            "23829bfc269042948448534c24709134",
            "c59603b6d2484a95b18f009a81d5bc66",
            "82a2000df7e543029f0964f399642d5d",
            "f4de1a432cd54914939b6b70e6af7dcc",
            "afb8e5c8b9ef4336bf6e2eee88d4e114",
            "ecd900fa3e114439822257ba77381f0a",
            "e6975b8856484e2f96d78fe4a23a8e10",
            "4bcfbe536a2d47a39d4293d998636243",
            "6cb7655e3cd541e5855462433aa534be",
            "74a8ce73ceb64b688147bb7a71d4c2c9",
            "494d0853e39641daa0805645ce6beb17",
            "34bf217aa66f4fb29d09f35f592c1bc9",
            "d49f51a25b964249852b50a2acc49634",
            "d6730505ea7d431fbfd80aa7a01dc790",
            "73a9344dc79e43f091c8d1b223cbcee4",
            "4e38bd08f29d40559bc7d339e03bd4c2",
            "8f3dde7227494dd9b0774ce28b388274",
            "0e3f8d3e5e5b4492a59f0d7d3186877e",
            "566df9d46f7943b7b672fe7dd9f7417c",
            "466b7df8811a413c95f08a580de6b89a",
            "75dbd3da373541b2bc952f7f6a164624",
            "e4231b41f41d421facafcdefb89969d7",
            "bd8e490fbdc5420f97d4c94f5e61b5c5",
            "4bc0fba67ddf478991c47aa6cab98ff4",
            "5d1434a6307d4ba9bed46f5d4ae730c0",
            "f470bf33537d462a883ef2d57df8bfbc",
            "25279ff26e0046899b173f33e3cea185",
            "24a0f86cb4504f6a88c4bcc9410a6882",
            "d6c63b0273ea460eb5e153e48101e332",
            "07238cfa6bd34bf0917a8622a1cd3b0d",
            "408079bc18cf4307b7a4aa0a81591979",
            "c541c5f56dd2474b84182f9703b4cda7"
          ]
        },
        "id": "rT78JNcqpRXW",
        "outputId": "72a24264-5b8c-4afe-ae65-c964a0b2fce2"
      },
      "outputs": [],
      "source": [
        "#dataset = load_dataset(\"IgorVolochay/russian_jokes\", data_files='dataset.csv')\n",
        "#print(\"\\n===\\n\".join(dataset[\"train\"][\"text\"][:3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7G7o6OdK99z",
        "outputId": "ddb5c277-107c-41f8-91cc-2eac3822bcd4"
      },
      "outputs": [],
      "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º —Ö–æ–ª–¥–∞—É—Ç—ã\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=SEED)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZzvdEVO3-kM"
      },
      "source": [
        "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä [6 –±–∞–ª–ª–æ–≤]\n",
        "\n",
        "–í –∫–∞—á–µ—Å—Ç–≤–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç Byte-level BPE.\n",
        "\n",
        "–î–ª—è —ç—Ç–æ–≥–æ:\n",
        "1. –†–µ–∞–ª–∏–∑—É–µ–º –µ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏ –Ω–∞–±–æ—Ä —Å–ª–∏—è–Ω–∏–π –ø–æ —ç—Ç–æ–º—É —Å–ª–æ–≤–∞—Ä—é\n",
        "2. –û–±—É—á–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
        "3. –†–µ–∞–ª–∏–∑—É–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15U6H1iLU3kI"
      },
      "outputs": [],
      "source": [
        "# –í—Å—è–∫–∏–µ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏\n",
        "\n",
        "WHITESPACE_SPLITTER = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "\n",
        "def bytes_to_unicode() -> dict[int, str]:\n",
        "    \"\"\"The original dictionary consists of 256 bytes and their corresponding Unicode characters.\n",
        "    For example, chr(33) is '!'. However, not all bytes have a visually appealing representation,\n",
        "    so such characters are skipped and replaced with the first available ones, i.e. shifted by 256.\n",
        "    \"\"\"\n",
        "    initial_bytes = (\n",
        "        list(range(ord(\"!\"), ord(\"~\") + 1)) +\n",
        "        list(range(ord(\"¬°\"), ord(\"¬¨\") + 1)) +\n",
        "        list(range(ord(\"¬Æ\"), ord(\"√ø\") + 1)) +\n",
        "        list(range(ord(\"–ê\"), ord(\"–Ø\") + 1)) +\n",
        "        list(range(ord(\"–∞\"), ord(\"—è\") + 1))\n",
        "    )\n",
        "    initial_chars = [chr(it) for it in initial_bytes]\n",
        "    n = 0\n",
        "    for byte in range(2**8):\n",
        "        if byte not in initial_bytes:\n",
        "            initial_bytes.append(byte)\n",
        "            initial_chars.append(chr(2**8 + n))\n",
        "            n += 1\n",
        "    return dict(sorted(zip(initial_bytes, initial_chars)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jsckv3BhORZQ",
        "outputId": "a64efb81-b03b-4412-e0b6-465f82b866ff"
      },
      "outputs": [],
      "source": [
        "d = bytes_to_unicode()\n",
        "print(d.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk919hENEFwL"
      },
      "outputs": [],
      "source": [
        "def merge(merge_pair: tuple[str, str], pair_frequences: Counter[tuple[str, str]], words_by_tokens: Counter[tuple[str]]):\n",
        "    \"\"\"Merges a given pair of tokens and update corresponding stats\n",
        "\n",
        "    Args:\n",
        "        merge_pair: The pair of tokens to be merged.\n",
        "        pair_frequences: A counter tracking the frequency of token pairs in the dataset.\n",
        "        words_by_tokens: A counter mapping tokenized words to their frequencies.\n",
        "\n",
        "    Returns:\n",
        "        Updated pair frequences and word tokenization w.r.t. to new token.\n",
        "    \"\"\"\n",
        "    new_words_by_tokens = Counter()\n",
        "\n",
        "    for word, freq in words_by_tokens.items():\n",
        "        new_word = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            if i < len(word) - 1 and (word[i], word[i + 1]) == merge_pair:\n",
        "                new_word.append(word[i] + word[i + 1])\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "        new_words_by_tokens[tuple(new_word)] += freq\n",
        "\n",
        "    new_pair_frequences = Counter()\n",
        "    for tokens, freq in new_words_by_tokens.items():\n",
        "        for i in range(len(tokens) - 1):\n",
        "            new_pair_frequences[(tokens[i], tokens[i+1])] += freq\n",
        "\n",
        "    return new_pair_frequences, new_words_by_tokens\n",
        "\n",
        "\n",
        "def train(data: list[str], vocab_size: int = 1024, special_tokens: list[str] = None):\n",
        "    \"\"\"Train BPE tokenizer on passed data\n",
        "\n",
        "    Args:\n",
        "        data: List of train documents\n",
        "        vocab_size: Size of target vocabulary\n",
        "        special_tokens: List of special tokens to add into vocabulary\n",
        "    Returns:\n",
        "        vocabulary: mapping from string token to id\n",
        "        merges: list of merges, each one is tuple of string tokens\n",
        "    \"\"\"\n",
        "    if vocab_size < 256:\n",
        "        raise ValueError(\"Vocab size can't be less than 256\")\n",
        "    if special_tokens is None:\n",
        "        special_tokens = []\n",
        "\n",
        "    # 1. Initialize vocabulary (using inverse one during training)\n",
        "    id2token = bytes_to_unicode()\n",
        "    merges = []\n",
        "\n",
        "    # 2. Load data\n",
        "    words_by_tokens = Counter()\n",
        "    for sample in tqdm(data, desc=\"Loading data\"):\n",
        "        # 2.1 Split into words\n",
        "        words = WHITESPACE_SPLITTER.findall(sample.strip())\n",
        "        for word in words:\n",
        "            # 2.2 Tokenize with base vocabulary\n",
        "            tokens = tuple(id2token[b] for b in word.encode('utf-8'))\n",
        "            words_by_tokens[tokens] += 1\n",
        "\n",
        "    # 3. Calculate statistic of token's pairs\n",
        "    pair_frequences = Counter()\n",
        "    for tokens, freq in words_by_tokens.items():\n",
        "        for i in range(len(tokens) - 1):\n",
        "            pair_frequences[(tokens[i], tokens[i+1])] += freq\n",
        "\n",
        "    # 4. Build vocabulary\n",
        "    pbar = trange(vocab_size, desc=\"Building vocabulary\", initial=len(id2token) + len(special_tokens))\n",
        "    while len(id2token) < vocab_size - len(special_tokens):\n",
        "        if len(pair_frequences) == 0:\n",
        "            print(\"Not enough data to fulfil vocabulary\")\n",
        "            break\n",
        "\n",
        "        # 4.1 Find the most frequent pair and create new token\n",
        "        top_pair = max(pair_frequences, key=pair_frequences.get)\n",
        "        new_token = top_pair[0] + top_pair[1]\n",
        "        del pair_frequences[top_pair]\n",
        "\n",
        "        # 4.2 Add to vocabulary\n",
        "        if new_token in id2token.values():\n",
        "            continue\n",
        "        id2token[len(id2token)] = new_token\n",
        "        merges.append(top_pair)\n",
        "\n",
        "        # 4.3 Update stats and merge the top pair in all tokens\n",
        "        pair_frequences, words_by_tokens = merge(top_pair, pair_frequences, words_by_tokens)\n",
        "\n",
        "        pbar.update()\n",
        "    pbar.close()\n",
        "\n",
        "    # 5. Add special tokens\n",
        "    for special_token in special_tokens:\n",
        "        id2token[len(id2token)] = special_token\n",
        "\n",
        "    return {v: k for k, v in id2token.items()}, merges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "a09b3fed74ed42efb5a3f71e5eae676c",
            "17262cca283248a98297387d6824d5bb",
            "51b3b59a23ad411988fc656d30d8fc8a",
            "1208ee3c939b4e18bcbf992a00bde69c",
            "4c1ebcad0d764e60a4948434d9993a2f",
            "ed5bfb7134314419a909b305e43d432b",
            "f0ae019189404e27a4286b05922c8f9e",
            "1a8fccc151714a19856de84a0416d3d8",
            "1e923b43f03f4bfe8c1315d8e98e049b",
            "513fd997a1444193bcaa161bb7c4e6b7",
            "963ec243a52f481d8e67977600e9b34d",
            "27d8bbba523844ce9568c9d8b6690cfa",
            "b4cf7e2c4a664fa398e6e60ae11e2348",
            "808c00da14224708918d15a5000434fc",
            "da58846238f54b6790acd3898da7b9a6",
            "880b38716d5f4eb5b4f7dfba3390dc45",
            "9fdad1a5749346b09daf54d983a22964",
            "9970c0d3b42b4ff08d31eeae1d0efe51",
            "86d07832c915484fa50db62c333922ec",
            "51fa888c7bf34a74a0742fc197d9487a",
            "7007abc4f18e4d7997aa1d7265c25eb4",
            "8142e932c2b94414bd27d4223c3a3d6c"
          ]
        },
        "id": "iLwur-KgK990",
        "outputId": "449b7f94-50ef-4a3c-eca6-620f8355d4c8"
      },
      "outputs": [],
      "source": [
        "# –û–±—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö\n",
        "# –î–ª—è –Ω–∞—à–µ–π –∑–∞–¥–∞—á–∏ —Ö–≤–∞—Ç–∏—Ç –∏ –Ω–µ–±–æ–ª—å—à–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è, –Ω–æ –º–æ–∂–µ—Ç–µ –ø—Ä–æ–±–æ–≤–∞—Ç—å –∏ –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –æ–±—É—á–∏—Ç—å!\n",
        "\n",
        "\n",
        "vocab, merges = train(dataset[\"train\"][\"text\"], vocab_size=1024, special_tokens=[\"[EOS]\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcsapJUAK990",
        "outputId": "29a6ecf6-84a4-42d1-f085-a414417e5a9a"
      },
      "outputs": [],
      "source": [
        "# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã\n",
        "\n",
        "random_tokens = [512, 614, 768, 888, 1022]\n",
        "unicode_to_bytes = {v: k for k, v in bytes_to_unicode().items()}\n",
        "for token_id in random_tokens:\n",
        "    token = [k for k, v in vocab.items() if v == token_id][0]\n",
        "    raw_bytes = bytes([unicode_to_bytes[it] for it in token])\n",
        "    print(f\"Token #{token_id}: '{raw_bytes.decode('utf-8', errors='replace')}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugUz7cma1Czs"
      },
      "outputs": [],
      "source": [
        "class ByteLevelBPETokenizer:\n",
        "\n",
        "    def __init__(self, vocab: dict[str, int], merges: list[tuple[str, str]], eos_token: str = \"[EOS]\"):\n",
        "        \"\"\"Byte-Level BPE Tokenizer\n",
        "\n",
        "        Args:\n",
        "            vocab: mapping from string token to id\n",
        "            merges: list of merges in prioritized order\n",
        "            eos_token: string representation of EOS token\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if eos_token not in vocab:\n",
        "            raise ValueError(\"There is no EOS token in vocab\")\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        self.token2id = vocab\n",
        "        self.id2token = {v: k for k, v in self.token2id.items()}\n",
        "        self.eos_token = eos_token\n",
        "        self.eos_token_id = self.token2id[eos_token]\n",
        "\n",
        "        # The closer the pair is to the beginning, the higher the rank\n",
        "        self.merges = merges\n",
        "        self.bpe_ranks = {pair: i for i, pair in enumerate(merges)}\n",
        "\n",
        "    def get_pairs(self, word: tuple[str]) -> set[tuple[str, str]]:\n",
        "        pairs = set()\n",
        "        for i in range(len(word) - 1):\n",
        "            pairs.add((word[i], word[i+1]))\n",
        "        return pairs\n",
        "\n",
        "\n",
        "    @lru_cache\n",
        "    def bpe(self, word: tuple[str]) -> tuple[str]:\n",
        "        \"\"\"Process word into tokenized representation.\n",
        "        Word is a tuple of base tokens, i.e. bytes.\n",
        "\n",
        "        Under the hood:\n",
        "        1. Tracks the set of token pairs, bi-grams\n",
        "        2. While possible, replaces the highest-ranking pair with its union\n",
        "\n",
        "        Args:\n",
        "            word: list of base string tokens\n",
        "        Return:\n",
        "            list of BPE tokens\n",
        "        \"\"\"\n",
        "        if len(word) == 1:\n",
        "            return word\n",
        "\n",
        "        word = list(word)\n",
        "        pairs = self.get_pairs(tuple(word))\n",
        "\n",
        "        while True:\n",
        "            candidate = None\n",
        "            min_rank = float('inf')\n",
        "            for pair in pairs:\n",
        "                if pair in self.bpe_ranks and self.bpe_ranks[pair] < min_rank:\n",
        "                    min_rank = self.bpe_ranks[pair]\n",
        "                    candidate = pair\n",
        "\n",
        "            if candidate is None:\n",
        "                break\n",
        "\n",
        "            first, second = candidate\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                except ValueError:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                new_word.extend(word[i:j])\n",
        "                if j < len(word) - 1 and word[j] == first and word[j+1] == second:\n",
        "                    new_word.append(first + second)\n",
        "                    i = j + 2\n",
        "                else:\n",
        "                    new_word.append(word[j])\n",
        "                    i = j + 1\n",
        "\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = self.get_pairs(tuple(word))\n",
        "        return tuple(word)\n",
        "\n",
        "    def encode(self, text: str, add_eos_token: bool = True) -> list[int]:\n",
        "        \"\"\"Convert string to list of token ids.\n",
        "\n",
        "        Args:\n",
        "            text: input string, may contain multiple words\n",
        "            add_eos_token: whether to add eos token id at the end\n",
        "        Return:\n",
        "            list of ints, ids of tokenized text\n",
        "        \"\"\"\n",
        "        words = WHITESPACE_SPLITTER.findall(text)\n",
        "        token_ids = []\n",
        "        for word in words:\n",
        "            word_bytes = word.encode('utf-8')\n",
        "            base_tokens = tuple(self.byte_encoder[b] for b in word_bytes)\n",
        "            bpe_tokens = self.bpe(base_tokens)\n",
        "            for token in bpe_tokens:\n",
        "                if token in self.token2id:\n",
        "                    token_ids.append(self.token2id[token])\n",
        "                else:\n",
        "                    for char in token:\n",
        "                        token_ids.append(self.token2id.get(char, -1))\n",
        "        if add_eos_token:\n",
        "            token_ids.append(self.eos_token_id)\n",
        "        return token_ids\n",
        "\n",
        "\n",
        "    def decode(self, idx: list[int]) -> str:\n",
        "        \"\"\"Convert list of tokens' ids to text, opposite to encode method\n",
        "\n",
        "        Args:\n",
        "            idx: list of tokens' ids\n",
        "        Return:\n",
        "            string, decoded text\n",
        "        \"\"\"\n",
        "        tokens = [self.id2token[i] for i in idx if i in self.id2token and self.id2token[i] != self.eos_token]\n",
        "        byte_list = []\n",
        "        for token in tokens:\n",
        "            for ch in token:\n",
        "                byte_list.append(self.byte_decoder.get(ch, ord('?')))\n",
        "        byte_string = bytes(byte_list)\n",
        "        return byte_string.decode('utf-8', errors='replace')\n",
        "\n",
        "    def push_to_hub(self, repo_id, *, private=None, token=None):\n",
        "        api = HfApi()\n",
        "        repo_id = api.create_repo(repo_id=repo_id, token=token, private=private, exist_ok=True).repo_id\n",
        "\n",
        "        # Push the files to the repo in a single commit\n",
        "        with SoftTemporaryDirectory() as tmp:\n",
        "            save_directory = Path(tmp) / repo_id\n",
        "            save_directory.mkdir(parents=True)\n",
        "            with open(save_directory / \"vocabulary.json\", \"w\") as f_out:\n",
        "                print(json.dumps(self.token2id, indent=2), file=f_out)\n",
        "            with open(save_directory / \"merges.json\", \"w\") as f_out:\n",
        "                print(json.dumps({\"merges\": self.merges}), file=f_out)\n",
        "\n",
        "            return api.upload_folder(repo_id=repo_id, folder_path=save_directory, token=token)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, *, token=None, **model_kwargs):\n",
        "        if not os.path.isdir(pretrained_model_name_or_path):\n",
        "            storage_folder = snapshot_download(repo_id=pretrained_model_name_or_path, token=token)\n",
        "        else:\n",
        "            storage_folder = pretrained_model_name_or_path\n",
        "        storage_folder = Path(storage_folder)\n",
        "        with open(storage_folder / \"vocabulary.json\", \"r\") as f_in:\n",
        "            vocab = json.load(f_in)\n",
        "        with open(storage_folder / \"merges.json\", \"r\") as f_in:\n",
        "            merges = [tuple(it) for it in json.load(f_in)[\"merges\"]]\n",
        "        return cls(vocab, merges, **model_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRTQzO3wK991"
      },
      "outputs": [],
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
        "\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(vocab, merges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "-9m_65vBK991",
        "outputId": "d30bc90d-e842-4438-95a6-968f4e16b93a"
      },
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ —Ö–∞–±\n",
        "\n",
        "tokenizer.push_to_hub(REPO_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ce6945736f7c4c839190e0404f370a1a",
            "91e13de6ce154bb09173c4f9f333611b",
            "f81001a64a784ae7a8d09fe258376e1d",
            "ffa04c096fc0464db8182467a307a2ae",
            "b976e96a647b4f40a50965b3cef1c8a5",
            "a4f01f1d458a453d8241de282a374b19",
            "b26a32b517324844b77e3de793b6c47d",
            "013f1b28161c4b2888981b2279309a08",
            "b9b9619cfb0f4451985c6466b0486fb9",
            "4b35e808896a4a37aab51a068f242c43",
            "b08ba319d12f4d15bd7ec8bea32a90f4"
          ]
        },
        "id": "S9VohtKrK991",
        "outputId": "c00aeffa-b063-4721-c948-8b4ad4c882f3"
      },
      "outputs": [],
      "source": [
        "# –°–∫–∞—á–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å —Ö–∞–±–∞\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer.from_pretrained(REPO_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFvf_Q8knPW4",
        "outputId": "76bb98fb-e441-43c4-cb1c-a873c00f8153"
      },
      "outputs": [],
      "source": [
        "# –°–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–∞–±–æ—Ç—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
        "\n",
        "text = \"–ß—Ç–æ –±—ã–ª–æ –ø–æ–ª–≥–æ–¥–∞ –Ω–∞–∑–∞–¥? –ü–æ–º–∏–º–æ –≥—Ä–∞–Ω–¥–∏–æ–∑–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π, –ø–æ–ª–≥–æ–¥–∞ –Ω–∞–∑–∞–¥ –±—ã–ª–∏ –µ—â—ë —Å–µ–º–∏–Ω–∞—Ä—ã –ø–æ –ª–∏–Ω–µ–π–Ω–æ–π –∞–ª–≥–µ–±—Ä–µ.\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n",
        "reverse_text = [tokenizer.decode([it]) for it in ids]\n",
        "print(\"|\".join(reverse_text))\n",
        "print(tokenizer.decode(ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "e5087d27c4f14a28bfb73740fa147d3a",
            "ef5bee8e8d2f4d8c91ba14e77dda8d01",
            "be6ceb6f27af40aba535c445806e297b",
            "f2a0e82016804d2ca04aeb19c0d936e5",
            "564c90bbde8442f59aa264ace4f8a90b",
            "1f3b2354ef974753bf0057ff07cdd208",
            "468d88309904488a817e7b81a392c8b3",
            "75ddc70c138c42389c14a293a68aade2",
            "946f2a1b3aa34be2a278e291fdb530e8",
            "f94a9f0d86294b0288ad87412ec2ca5d",
            "69fa3278c9ff402784c86c44d74e3e70"
          ]
        },
        "id": "_QgpwFYiK991",
        "outputId": "3a0efb91-2cff-4fc6-b029-adeda727f4fe"
      },
      "outputs": [],
      "source": [
        "# –ü–æ—Å—á–∏—Ç–∞–µ–º –Ω–µ–º–Ω–æ–≥–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –æ–ø—Ä–µ–¥–µ–ª–∏–º—Å—è —Å —Ä–∞–∑–º–µ—Ä–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —É –º–æ–¥–µ–ª–∏\n",
        "\n",
        "lens = []\n",
        "for text in tqdm(dataset[\"test\"][\"text\"]):\n",
        "    ids = tokenizer.encode(text)\n",
        "    lens.append(len(ids))\n",
        "\n",
        "print(f\"Average token len per sample: {sum(lens) / len(lens):.2f}\")\n",
        "print(f\"Minimum and maximum lens are: {min(lens)} and {max(lens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwN3mfmznPW5"
      },
      "source": [
        "–î–æ–ª–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å—Å—è –≤ —Å—Ä–µ–¥–Ω–µ–º –ø–æ 70 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å.\n",
        "–ö–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ 128 —Ç–æ–∫–µ–Ω–æ–≤ –±—É–¥–µ—Ç –≤–ø–æ–ª–Ω–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub1FljGXC7I-"
      },
      "source": [
        "# –ú–æ–¥–µ–ª—å [10 –±–∞–ª–ª–æ–≤]\n",
        "\n",
        "–í –∫–∞—á–µ—Å—Ç–≤–µ –º–æ–¥–µ–ª–∏ —Ä–µ–∞–ª–∏–∑—É–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –≤ –∫–æ—Ç–æ—Ä–æ–º\n",
        "1. –í –∫–∞—á–µ—Å—Ç–≤–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è ALiBi\n",
        "2. –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GQA\n",
        "3. –í Feed-Forward –±–ª–æ–∫–µ SwiGLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipZrCvkmK992"
      },
      "outputs": [],
      "source": [
        "# –î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –∑–∞–≤–µ–¥–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –º–æ–¥–µ–ª–∏\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TransformerConfig:\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_kv_head: int\n",
        "    hidden_dim: int\n",
        "    intermediate_dim: int\n",
        "    dropout: float = 0.1\n",
        "    vocab_size: int = 1024\n",
        "    max_seq_len: int = 128\n",
        "\n",
        "\n",
        "model_configs = {\n",
        "    \"nano\": TransformerConfig(n_layer=3, n_head=4, n_kv_head=2, hidden_dim=96, intermediate_dim=256),\n",
        "    \"mini\": TransformerConfig(n_layer=6, n_head=6, n_kv_head=3, hidden_dim=384, intermediate_dim=1024),\n",
        "    \"small\": TransformerConfig(n_layer=12, n_head=12, n_kv_head=6, hidden_dim=768, intermediate_dim=2048),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYXMb5PEDFkt"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        \"\"\"Root Mean Square Layer Normalization\n",
        "\n",
        "        Args:\n",
        "            dim: Feature dimension\n",
        "            eps: Small constant for numerical stability\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.scale = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        rms = torch.sqrt((x ** 2).mean(-1, keepdim=True) + self.eps)\n",
        "        return self.scale * x / rms\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        \"\"\"Causal Self-Attention with support of\n",
        "        Grouped-Query Attention and ALiBi for positional encoding\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        assert self.config.hidden_dim % self.config.n_head == 0 \n",
        "        assert self.config.n_head % self.config.n_kv_head == 0\n",
        "        self.head_dim = self.config.hidden_dim // self.config.n_head\n",
        "        self.scale = self.head_dim**-0.5\n",
        "        self.q_per_kv = self.config.n_head // self.config.n_kv_head\n",
        "\n",
        "        # Init projection layers\n",
        "        self.q_proj = nn.Linear(self.config.hidden_dim, self.config.hidden_dim)\n",
        "        self.kv_proj = nn.Linear(self.config.hidden_dim, self.config.n_kv_head * 2 * self.head_dim)\n",
        "        self.out_proj = nn.Linear(self.config.hidden_dim, self.config.hidden_dim)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(self.config.dropout)\n",
        "\n",
        "        self.register_buffer(\"causal_mask\", self._create_causal_mask(self.config.max_seq_len))\n",
        "        self.register_buffer(\"alibi\", self._build_alibi_bias(self.config.n_head))\n",
        "\n",
        "    def _build_alibi_bias(self, num_heads: int) -> Tensor:\n",
        "        \"\"\"Build ALiBi for specified number of heads:\n",
        "\n",
        "        Returns:\n",
        "            Tensor with ALiBi biases, shape: [1, num heads, 1, 1]\n",
        "        \"\"\"\n",
        "        slopes = torch.pow(2, -torch.arange(0, num_heads, dtype=torch.float32) / num_heads)\n",
        "        return slopes.view(1, num_heads, 1, 1)\n",
        "    \n",
        "    def _create_causal_mask(self, max_seq_len: int) -> Tensor:\n",
        "        \"\"\"Create causal mask with ones where tokens can attend to each other.\n",
        "\n",
        "        Returns:\n",
        "            Tensor with causal mask, shape: [1, 1, seq len, seq len]\n",
        "        \"\"\"\n",
        "        tri_mask = torch.tril(torch.ones((max_seq_len, max_seq_len), dtype=torch.bool))\n",
        "        return tri_mask.view(1, 1, max_seq_len, max_seq_len)\n",
        "\n",
        "    def forward(self, x: Tensor, attention_mask: Tensor = None) -> Tensor:\n",
        "        \"\"\"Apply Self-Attention to input data with respect to pad tokens.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor, shape [bs, seq len, hidden dim]\n",
        "            attention_mask: mask with zeros for pad tokens, shape [bs, seq len, hidden dim]\n",
        "        Returns:\n",
        "            result tensor, shape [bs, seq len, hidden dim]\n",
        "        \"\"\"\n",
        "        bs, seq_len, hidden_dim = x.size()\n",
        "\n",
        "        q = self.q_proj(x)\n",
        "        kv = self.kv_proj(x)\n",
        "        k, v = kv.chunk(2, dim=-1) \n",
        "\n",
        "        # –ø–µ—Ä–µ—Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∏—Ö\n",
        "        q = q.view(bs, seq_len, self.config.n_head, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(bs, seq_len, self.config.n_kv_head, self.head_dim)\n",
        "        v = v.view(bs, seq_len, self.config.n_kv_head, self.head_dim)\n",
        "        k = k.repeat_interleave(self.q_per_kv, dim=2).transpose(1, 2)\n",
        "        v = v.repeat_interleave(self.q_per_kv, dim=2).transpose(1, 2)\n",
        "\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        # –¥–æ–±–∞–≤–∏–º –ø–æ–∑–∏—à–Ω–∞–ª —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
        "        pos = torch.arange(seq_len, device=x.device)\n",
        "        rel_pos = pos.view(1, -1) - pos.view(-1, 1)\n",
        "        alibi_bias = -self.alibi * rel_pos.unsqueeze(0).unsqueeze(0).float()\n",
        "        attn_scores = attn_scores + alibi_bias\n",
        "\n",
        "        causal_mask = self.causal_mask[:, :, :seq_len, :seq_len]\n",
        "        attn_scores = attn_scores.masked_fill(~causal_mask, float('-inf'))\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(attention_mask.unsqueeze(1).unsqueeze(2) == 0, float('-inf'))\n",
        "\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        attn_probs = self.attn_dropout(attn_probs)\n",
        "\n",
        "        context = torch.matmul(attn_probs, v)\n",
        "        context = context.transpose(1, 2).contiguous().view(bs, seq_len, self.config.hidden_dim)\n",
        "\n",
        "        output = self.out_proj(context)\n",
        "        return output\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        \"\"\"Gated Liner Unit with Swish Activation\"\"\"\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        # Init up- and down- projection layers\n",
        "        self.fc1 = nn.Linear(config.hidden_dim, config.intermediate_dim * 2)\n",
        "        self.fc2 = nn.Linear(config.intermediate_dim, config.hidden_dim)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Apply SwiGLU to input data.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor, shape [bs, seq len, hidden dim]\n",
        "        Returns:\n",
        "            result tensor, shape [bs, seq len, hidden dim]\n",
        "        \"\"\"\n",
        "        x_proj = self.fc1(x)\n",
        "        x1, x2 = x_proj.chunk(2, dim=-1)  # —Ä–∞–∑–¥–µ–ª—è–µ–º\n",
        "        x2 = x2 * torch.sigmoid(x2) # swish: x2 * sigmoid(x2)\n",
        "        gated = x1 * x2\n",
        "        return self.fc2(gated)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        \"\"\"Base Transformer Block\n",
        "        - Causal Self-Attention and SwiGLU as main elements\n",
        "        - Pre-normalization via RMSNorm\n",
        "        - Regularization with dropouts before residuals\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.ln_1 = RMSNorm(config.hidden_dim)\n",
        "        self.res_dropout_1 = nn.Dropout(config.dropout)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "\n",
        "        self.ln_2 = RMSNorm(config.hidden_dim)\n",
        "        self.res_dropout_2 = nn.Dropout(config.dropout)\n",
        "        self.mlp = SwiGLU(config)\n",
        "\n",
        "    def forward(self, x: Tensor, attention_mask: Tensor = None) -> Tensor:\n",
        "        \"\"\"Apply Transformer Block to input data.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor, shape [bs, seq len, hidden dim]\n",
        "            attention_mask: mask with zeros for pad tokens, shape [bs, seq len, hidden dim]\n",
        "        Returns:\n",
        "            result tensor, shape [bs, seq len, hidden dim]\n",
        "        \"\"\"\n",
        "        x_attn = self.attn(self.ln_1(x), attention_mask)\n",
        "        x = x + self.res_dropout_1(x_attn)\n",
        "\n",
        "        x_ff = self.mlp(self.ln_2(x))\n",
        "        x = x + self.res_dropout_2(x_ff)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerForCausalLM(nn.Module, PyTorchModelHubMixin):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        \"\"\"Transformer model for Language Modeling\"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.max_seq_len = config.max_seq_len\n",
        "        self.n_layer = config.n_layer\n",
        "        self.n_head = config.n_head\n",
        "        self.hidden_dim = config.hidden_dim\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "        self.token_emb = nn.Embedding(self.vocab_size, self.hidden_dim)\n",
        "        self.emb_dropout = nn.Dropout(config.dropout)\n",
        "        self.layers = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_final = RMSNorm(config.hidden_dim)\n",
        "        self.lm_head = nn.Linear(self.hidden_dim, self.vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"Number of parameters: {n_params / 1e6:.2f}M\")\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, RMSNorm):\n",
        "            torch.nn.init.ones_(module.scale)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None) -> Tensor:\n",
        "        \"\"\"Calculate logits for given input ids.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor, shape [bs, seq len, hidden dim]\n",
        "            attention_mask: mask with zeros for pad tokens, shape [bs, seq len, hidden dim]\n",
        "        Returns:\n",
        "            logits, shape [bs, seq len, hidden dim]\n",
        "        \"\"\"\n",
        "        x = self.token_emb(input_ids)\n",
        "        x = self.emb_dropout(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attention_mask)\n",
        "        x = self.ln_final(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate(\n",
        "        self, idx: Tensor, max_new_tokens, eos_token_id, temperature=1.0, do_sample=False, top_k=None\n",
        "    ) -> Tensor:\n",
        "        \"\"\"Take a conditioning sequence of indices and complete the sequence max_new_tokens times,\n",
        "        feeding the predictions back into the model each time.\n",
        "\n",
        "        Args:\n",
        "            idx: tensor with conditional tokens, shape [seq len]\n",
        "            max_new_tokens: maximum number of new tokens\n",
        "            eos_token_id: index of EOS token to stop generation\n",
        "            temperature, do_sample, top_k: generation parameters\n",
        "        Return:\n",
        "            tensor with generated indexes\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.shape[1] <= self.max_seq_len else idx[:, -self.max_seq_len :]\n",
        "            logits = self(idx_cond)\n",
        "\n",
        "            # 1. Pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :]  # [bs, vocab_size]\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # 2. Optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                kth_value = torch.topk(logits, top_k, dim=-1)[0][..., -1, None]\n",
        "                mask = logits < kth_value\n",
        "                logits[mask] = float(\"-inf\")\n",
        "\n",
        "            # 3. apply softmax to convert logits to probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            # 4. Either sample from the distribution or take the most likely element\n",
        "            if do_sample:\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "\n",
        "            # 5. Append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "            if idx_next == eos_token_id:\n",
        "                break\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –ú–æ—è –ø–æ–ø—ã—Ç–∫–∞ –¥–æ–±–∞–≤–∏—Ç—å –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ RoPE –∏ MHLA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MHLASelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        \"\"\"Self-Attention with support of\n",
        "            Multi-Headed Linear Attention and RoPE for positional encoding\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        assert self.config.hidden_dim % self.config.n_head == 0 \n",
        "        assert self.config.n_head % self.config.n_kv_head == 0\n",
        "        self.head_dim = self.config.hidden_dim // self.config.n_head\n",
        "        self.q_per_kv = self.config.n_head // self.config.n_kv_head\n",
        "\n",
        "        self.q_proj = nn.Linear(self.config.hidden_dim, self.config.hidden_dim)\n",
        "        self.kv_proj = nn.Linear(self.config.hidden_dim, self.config.n_kv_head * 2 * self.head_dim)\n",
        "        self.out_proj = nn.Linear(self.config.hidden_dim, self.config.hidden_dim)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(self.config.dropout)\n",
        "\n",
        "        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±—É—Ñ–µ—Ä–æ–≤ –¥–ª—è RoPE\n",
        "        max_seq_len = self.config.max_seq_len\n",
        "        d = self.head_dim\n",
        "        \n",
        "        assert d % 2 == 0, \"head_dim –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–µ—Ç–Ω—ã–º –¥–ª—è RoPE\"\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, d, 2, dtype=torch.float32) / d))\n",
        "        positions = torch.arange(max_seq_len, dtype=torch.float32)\n",
        "        sinusoid_inp = torch.einsum(\"i,j->ij\", positions, inv_freq)\n",
        "        cos = torch.cos(sinusoid_inp)\n",
        "        sin = torch.sin(sinusoid_inp)\n",
        "        # —Å–æ—Ö—Ä–∞–Ω–∏–º –±—É—Ñ–µ—Ä—ã —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∏–∑–º–µ—Ä–µ–Ω–∏—è–º–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è:\n",
        "        self.register_buffer(\"rope_cos\", cos.unsqueeze(1))\n",
        "        self.register_buffer(\"rope_sin\", sin.unsqueeze(1))\n",
        "\n",
        "    def apply_rope(self, x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        –ü—Ä–∏–º–µ–Ω—è–µ—Ç Rotary Positional Embedding –∫ —Ç–µ–Ω–∑–æ—Ä—É x.\n",
        "            x:        [B, n_head, seq_len, head_dim]\n",
        "            cos, sin: [1, 1, seq_len, head_dim/2]\n",
        "        \"\"\"\n",
        "        x1 = x[..., ::2]\n",
        "        x2 = x[..., 1::2]\n",
        "        # [x1*cos - x2*sin, x1*sin + x2*cos]\n",
        "        x_rotated = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
        "        return x_rotated\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "            x: [batch_size, seq_len, hidden_dim]\n",
        "            attention_mask –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, —Ç–∞–∫ –∫–∞–∫ –∫–∞—É–∑–∞–ª—å–Ω–æ—Å—Ç—å –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ.\n",
        "        \"\"\"\n",
        "        bs, seq_len, _ = x.size()\n",
        "\n",
        "        q = self.q_proj(x)\n",
        "        kv = self.kv_proj(x)\n",
        "        k, v = kv.chunk(2, dim=-1)\n",
        "\n",
        "        q = q.view(bs, seq_len, self.config.n_head, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(bs, seq_len, self.config.n_kv_head, self.head_dim)\n",
        "        v = v.view(bs, seq_len, self.config.n_kv_head, self.head_dim)\n",
        "\n",
        "        k = k.repeat_interleave(self.q_per_kv, dim=2).transpose(1, 2)\n",
        "        v = v.repeat_interleave(self.q_per_kv, dim=2).transpose(1, 2) \n",
        "\n",
        "        cos = self.rope_cos[:seq_len].unsqueeze(0).transpose(1, 2)\n",
        "        sin = self.rope_sin[:seq_len].unsqueeze(0).transpose(1, 2)\n",
        "        q = self.apply_rope(q, cos, sin)\n",
        "        k = self.apply_rope(k, cos, sin)\n",
        "\n",
        "        # –ø—Ä–∏–º–µ–Ω—è–µ–º phi: phi(x) = elu(x) + 1\n",
        "        phi_q = F.elu(q) + 1\n",
        "        phi_k = F.elu(k) + 1\n",
        "\n",
        "        # —Ç–µ–ø–µ—Ä—å —Ä–µ–∞–ª–∏–∑—É–µ–º MHLA —á–µ—Ä–µ–∑ –∫–∞—É–∑–∞–ª—å–Ω–æ–µ –ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ\n",
        "        outputs = []\n",
        "        S = None  # –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ outer-–ø—Ä–æ–¥—É–∫—Ç–∞\n",
        "        Z = None  # –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ phi(k)\n",
        "        eps = 1e-6\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            phi_k_i = phi_k[:, :, i, :]\n",
        "            v_i = v[:, :, i, :]\n",
        "            # –≤—ã—á–∏—Å–ª—è–µ–º outer-–ø—Ä–æ–¥—É–∫—Ç –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —à–∞–≥–∞\n",
        "            outer = torch.einsum(\"bnd,bne->bnde\", phi_k_i, v_i)\n",
        "            if i == 0:\n",
        "                S = outer\n",
        "                Z = phi_k_i\n",
        "            else:\n",
        "                S = S + outer\n",
        "                Z = Z + phi_k_i\n",
        "\n",
        "            phi_q_i = phi_q[:, :, i, :]\n",
        "            numerator = torch.einsum(\"bnd,bnde->bne\", phi_q_i, S)\n",
        "            denominator = torch.sum(phi_q_i * Z, dim=-1, keepdim=True)\n",
        "            output_i = numerator / (denominator + eps)\n",
        "            outputs.append(output_i)\n",
        "            \n",
        "        # c–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏, —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≥–æ–ª–æ–≤—ã\n",
        "        out = torch.stack(outputs, dim=2)\n",
        "        out = self.attn_dropout(out)\n",
        "        out = out.transpose(1, 2).contiguous().view(bs, seq_len, self.config.hidden_dim)\n",
        "        output = self.out_proj(out)\n",
        "        return output\n",
        "    \n",
        "    \n",
        "class ModifiedBlock(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        \"\"\"Base Transformer Block\n",
        "        - Causal Self-Attention and SwiGLU as main elements\n",
        "        - Pre-normalization via RMSNorm\n",
        "        - Regularization with dropouts before residuals\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.ln_1 = RMSNorm(config.hidden_dim)\n",
        "        self.res_dropout_1 = nn.Dropout(config.dropout)\n",
        "        self.attn = MHLASelfAttention(config)\n",
        "\n",
        "        self.ln_2 = RMSNorm(config.hidden_dim)\n",
        "        self.res_dropout_2 = nn.Dropout(config.dropout)\n",
        "        self.mlp = SwiGLU(config)\n",
        "\n",
        "    def forward(self, x: Tensor, attention_mask: Tensor = None) -> Tensor:\n",
        "        \"\"\"Apply Transformer Block to input data.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor, shape [bs, seq len, hidden dim]\n",
        "            attention_mask: mask with zeros for pad tokens, shape [bs, seq len, hidden dim]\n",
        "        Returns:\n",
        "            result tensor, shape [bs, seq len, hidden dim]\n",
        "        \"\"\"\n",
        "        x_attn = self.attn(self.ln_1(x), attention_mask)\n",
        "        x = x + self.res_dropout_1(x_attn)\n",
        "\n",
        "        x_ff = self.mlp(self.ln_2(x))\n",
        "        x = x + self.res_dropout_2(x_ff)\n",
        "        return x\n",
        "    \n",
        "\n",
        "class ModifiedTransformerForCausalLM(nn.Module, PyTorchModelHubMixin):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        \"\"\"Transformer model for Language Modeling\"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.max_seq_len = config.max_seq_len\n",
        "        self.n_layer = config.n_layer\n",
        "        self.n_head = config.n_head\n",
        "        self.hidden_dim = config.hidden_dim\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "        self.token_emb = nn.Embedding(self.vocab_size, self.hidden_dim)\n",
        "        self.emb_dropout = nn.Dropout(config.dropout)\n",
        "        self.layers = nn.ModuleList([ModifiedBlock(config) for _ in range(config.n_layer)])\n",
        "        self.ln_final = RMSNorm(config.hidden_dim)\n",
        "        self.lm_head = nn.Linear(self.hidden_dim, self.vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"Number of parameters: {n_params / 1e6:.2f}M\")\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, RMSNorm):\n",
        "            torch.nn.init.ones_(module.scale)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None) -> Tensor:\n",
        "        \"\"\"Calculate logits for given input ids.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor, shape [bs, seq len, hidden dim]\n",
        "            attention_mask: mask with zeros for pad tokens, shape [bs, seq len, hidden dim]\n",
        "        Returns:\n",
        "            logits, shape [bs, seq len, hidden dim]\n",
        "        \"\"\"\n",
        "        x = self.token_emb(input_ids)\n",
        "        x = self.emb_dropout(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attention_mask)\n",
        "        x = self.ln_final(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate(\n",
        "        self, idx: Tensor, max_new_tokens, eos_token_id, temperature=1.0, do_sample=False, top_k=None\n",
        "    ) -> Tensor:\n",
        "        \"\"\"Take a conditioning sequence of indices and complete the sequence max_new_tokens times,\n",
        "        feeding the predictions back into the model each time.\n",
        "\n",
        "        Args:\n",
        "            idx: tensor with conditional tokens, shape [seq len]\n",
        "            max_new_tokens: maximum number of new tokens\n",
        "            eos_token_id: index of EOS token to stop generation\n",
        "            temperature, do_sample, top_k: generation parameters\n",
        "        Return:\n",
        "            tensor with generated indexes\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.shape[1] <= self.max_seq_len else idx[:, -self.max_seq_len :]\n",
        "            logits = self(idx_cond)\n",
        "\n",
        "            # 1. Pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :]  # [bs, vocab_size]\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # 2. Optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                kth_value = torch.topk(logits, top_k, dim=-1)[0][..., -1, None]\n",
        "                mask = logits < kth_value\n",
        "                logits[mask] = float(\"-inf\")\n",
        "\n",
        "            # 3. apply softmax to convert logits to probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            # 4. Either sample from the distribution or take the most likely element\n",
        "            if do_sample:\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "\n",
        "            # 5. Append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "            if idx_next == eos_token_id:\n",
        "                break\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhdavMmSDujw"
      },
      "source": [
        "# Train Loop [2 + 2 –±–∞–ª–ª–∞]\n",
        "\n",
        "–ù–∞—Å—Ç–∞–ª–æ –≤—Ä–µ–º—è –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å.\n",
        "–ù–µ–±–æ–ª—å—à—É—é –º–æ–∂–Ω–æ –ø—Ä–æ–±–æ–≤–∞—Ç—å –æ–±—É—á–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ, –Ω–æ –ª—É—á—à–µ –≤—Å–µ–≥–æ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è GPU, –Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞ Google Colab.\n",
        "\n",
        "–ó–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é 2 –±–∞–ª–ª–∞, –∏ –µ—â–µ 2 –±–∞–ª–ª–∞ - –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–∞—É—á–∏–ª–∞—Å—å –≥–µ–Ω–µ—Ä–∏—Ç—å –∞–Ω–µ–∫–¥–æ—Ç—ã.\n",
        "\n",
        "–ù–µ –∑–∞–±—É–¥—å—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –≤—ã –∑–∞–≥—Ä—É–∑–∏–ª–∏ –Ω—É–∂–Ω—ã–µ –≤–µ—Å–∞ –Ω–∞ HF –∏ —É –ø—Ä–æ–≤–µ—Ä—è—é—â–µ–≥–æ —Å–∫–∞—á–∞–µ—Ç—Å—è –Ω—É–∂–Ω–∞—è –≤–µ—Ä—Å–∏—è."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wtAiR_aDxed",
        "outputId": "1e5ecf77-0a8c-4b79-be67-1fe9f54cc4ce"
      },
      "outputs": [],
      "source": [
        "# –û–ø—Ä–µ–¥–µ–ª–∏–º –¥–∞—Ç–∞—Å–µ—Ç –∏ –∫–∞–∫ –∑–∞–≤–æ—Ä–∞—á–∏–≤–∞—Ç—å —Å–µ–º–ø–ª—ã –≤ –±–∞—Ç—á\n",
        "# –†–∞–∑–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –∏–º–µ—é—Ç —Ä–∞–∑–Ω—É—é –¥–ª–∏–Ω—É, –ø–æ—ç—Ç–æ–º—É –±—É–¥–µ—Ç –ø–∞–¥–∏—Ç—å –¥–æ —Å–∞–º–æ–≥–æ –¥–ª–∏–Ω–∞ —Å–µ–º–ø–ª–∞\n",
        "# –¢–∞–∫ –∂–µ –∑–∞–≤–µ–¥–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –º–∞—Å–∫—É, —á—Ç–æ–±—ã –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –Ω–µ —É—á–∏—Ç—ã–≤–∞–ª –ø–∞–¥–∏–Ω–≥–∏\n",
        "\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        texts = self.texts[idx]\n",
        "        tokenized_sequence = self.tokenizer.encode(texts)\n",
        "        return tokenized_sequence\n",
        "\n",
        "\n",
        "def data_collator(\n",
        "    tokenized_sequences: list[list[int]], pad_token_id: int, max_seq_len: int = None\n",
        ") -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    batch_size = len(tokenized_sequences)\n",
        "    max_batch_seq_len = min(max_seq_len, max((len(it) for it in tokenized_sequences)))\n",
        "\n",
        "    input_ids = torch.full((batch_size, max_batch_seq_len), pad_token_id)\n",
        "    attention_mask = torch.zeros((batch_size, max_batch_seq_len))\n",
        "\n",
        "    for i, tok_seq in enumerate(tokenized_sequences):\n",
        "        cur_len = min(len(tok_seq), max_batch_seq_len)\n",
        "        input_ids[i, :cur_len] = torch.tensor(tok_seq[:cur_len])\n",
        "        attention_mask[i, :cur_len] = 1\n",
        "\n",
        "    return input_ids, attention_mask\n",
        "\n",
        "\n",
        "def create_dataloader(dataset, pad_token_id, max_seq_len, batch_size, is_train):\n",
        "    collate_fn = partial(data_collator, pad_token_id=pad_token_id, max_seq_len=max_seq_len)\n",
        "    return DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=is_train, drop_last=is_train, collate_fn=collate_fn, pin_memory=True\n",
        "    )\n",
        "\n",
        "\n",
        "_d = TextDataset([\"–ü—Ä–∏–≤–µ—Ç!\", \"–ö–∞–∫ —Ç–≤–æ–∏ –¥–µ–ª–∞?\", \"–û—Å—Ç–∞–ª–æ—Å—å —Å–æ–≤—Å–µ–º –Ω–µ–º–Ω–æ–≥–æ –¥–æ –∫–æ–Ω—Ü–∞\"], tokenizer)\n",
        "_dl = create_dataloader(_d, tokenizer.eos_token_id, max_seq_len=16, batch_size=2, is_train=False)\n",
        "\n",
        "for i, batch in enumerate(_dl):\n",
        "    print(f\"Batch #{i}\")\n",
        "    input_ids, attn_mask = batch\n",
        "    print(input_ids, attn_mask, sep=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i719AOdQK993"
      },
      "outputs": [],
      "source": [
        "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
        "    \"\"\"Scheduler for Optimizer with linear warmup and linear decay to the end of training\n",
        "\n",
        "    Args:\n",
        "        optimizer: torch optimizer to control learning rate\n",
        "        num_warmup_steps: number of warmup steps\n",
        "        num_training_steps: total number of training steps\n",
        "    Return:\n",
        "        torch learning rate scheduler\n",
        "    \"\"\"\n",
        "    assert num_training_steps >= num_warmup_steps\n",
        "\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        else:\n",
        "            return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "\n",
        "\n",
        "def cross_entropy_loss(input_ids: Tensor, attention_mask: Tensor, logits: Tensor) -> Tensor:\n",
        "    \"\"\"Calculate Cross-Entropy loss for Language Modeling task\n",
        "    Under the hood:\n",
        "    1. Create targtes based on input ids\n",
        "    2. Masked out tokens corresponded to paddings\n",
        "    3. Calculate cross entropy loss\n",
        "\n",
        "    Args:\n",
        "        input_ids: tensor with input ids, shape [bs, seq len]\n",
        "        attention_mask: mask with zeros for pad tokens, shape [bs, seq len]\n",
        "        logits: predicted logits, shape [bs, seq len, vocab size]\n",
        "    Return:\n",
        "        cross entropy loss, single-item tensor\n",
        "    \"\"\"\n",
        "    # –æ—Å—Ç–∞–≤–ª—è–µ–º –ª–æ–≥–∏—Ç—ã —Å–æ —Å–¥–≤–∏–≥–æ–º –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
        "    logits = logits[:, :-1, :]\n",
        "    targets = input_ids[:, 1:]\n",
        "    target_mask = attention_mask[:, 1:]\n",
        "\n",
        "    targets = targets.clone()\n",
        "    targets[target_mask == 0] = -100\n",
        "\n",
        "    logits_flat = logits.reshape(-1, logits.size(-1))\n",
        "    targets_flat = targets.reshape(-1)\n",
        "    \n",
        "    loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat, ignore_index=-100)\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPYdF52zXtoX"
      },
      "outputs": [],
      "source": [
        "# –û–ø—Ä–µ–¥–µ–ª–∏–º —Ç—Ä–µ–Ω–µ—Ä–∞ —Å –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        learning_rate=3e-4,\n",
        "        weight_decay=0.01,\n",
        "        clip_grad_norm=1.0,\n",
        "        n_steps=8_000,\n",
        "        val_every_n_steps=1_000,\n",
        "        plot_every_n_steps=100,\n",
        "    ):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.clip_grad_norm = clip_grad_norm\n",
        "        self.n_steps = n_steps\n",
        "        self.val_every_n_steps = val_every_n_steps\n",
        "        self.plot_every_n_steps = plot_every_n_steps\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = \"cuda\"\n",
        "        elif torch.backends.mps.is_available():\n",
        "            self.device = \"mps\"\n",
        "        else:\n",
        "            self.device = \"cpu\"\n",
        "        print(\"running on device\", self.device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self, model, val_loader):\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
        "            input_ids, attention_mask = batch\n",
        "            input_ids = input_ids.to(self.device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)  # [bs; seq len; vocab size]\n",
        "            val_loss += cross_entropy_loss(input_ids, attention_mask, logits)\n",
        "        return val_loss / len(val_loader)\n",
        "\n",
        "    def run(self, model, train_loader, val_loader):\n",
        "        model = model.to(self.device)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=0.1 * self.n_steps, num_training_steps=self.n_steps\n",
        "        )\n",
        "        model.train()\n",
        "\n",
        "        plotlosses = PlotLosses(figsize=(15, 9), step_names=\"Step\")\n",
        "        logs = {\"lr\": 0, \"epoch\": 0}\n",
        "\n",
        "        data_iter = iter(train_loader)\n",
        "        for iter_num in range(self.n_steps):\n",
        "            try:\n",
        "                batch = next(data_iter)\n",
        "            except StopIteration:\n",
        "                data_iter = iter(train_loader)\n",
        "                logs[\"epoch\"] += 1\n",
        "                batch = next(data_iter)\n",
        "\n",
        "            input_ids, attention_mask = batch\n",
        "            input_ids = input_ids.to(self.device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)  # [bs; seq len; vocab size]\n",
        "            loss = cross_entropy_loss(input_ids, attention_mask, logits)\n",
        "\n",
        "            # backprop and update the parameters\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), self.clip_grad_norm)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            if iter_num > 0 and iter_num % self.val_every_n_steps == 0:\n",
        "                val_loss = self.validate(model, val_loader)\n",
        "                plotlosses.update({\"val_loss\": val_loss.item()}, current_step=iter_num)\n",
        "                plotlosses.send()\n",
        "                model.train()\n",
        "\n",
        "            if iter_num % self.plot_every_n_steps == 0:\n",
        "                logs[\"loss\"] = loss.item()\n",
        "                logs[\"lr\"] = scheduler.get_last_lr()[0]\n",
        "                plotlosses.update(logs, current_step=iter_num)\n",
        "                plotlosses.send()\n",
        "\n",
        "        val_loss = self.validate(model, val_loader)\n",
        "        plotlosses.update({\"val_loss\": val_loss.item()}, current_step=iter_num)\n",
        "        plotlosses.send()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK1BpJflMTAi"
      },
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä—ã\n",
        "\n",
        "\n",
        "MAX_SEQ_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataset = TextDataset(dataset[\"train\"][\"text\"], tokenizer)\n",
        "train_dataloader = create_dataloader(\n",
        "    train_dataset, tokenizer.eos_token_id, max_seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE, is_train=True\n",
        ")\n",
        "\n",
        "test_dataset = TextDataset(dataset[\"test\"][\"text\"], tokenizer)\n",
        "test_dataloader = create_dataloader(\n",
        "    test_dataset, tokenizer.eos_token_id, max_seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE, is_train=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53jHSgMZECGl",
        "outputId": "8eb387fb-d203-4270-eaa3-938df6c67b90"
      },
      "outputs": [],
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å\n",
        "\n",
        "config = model_configs[\"mini\"]\n",
        "model = TransformerForCausalLM(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "–Ø –ø–æ–ø—ã—Ç–∞–ª—Å—è —Ä–µ–∞–ª–∏–∑—Ä–æ–≤–∞—Ç—å, –ª–æ—Å—Å –ø–∞–¥–∞–µ—Ç \"–±—ã—Å—Ç—Ä–µ–µ\", –Ω–æ —è —É—Å–ø–µ–ª –æ–±—É—á–∏—Ç—å —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º —Ç–æ–ª—å–∫–æ nano –º–æ–¥–µ–ª—å, –∏ —Ç–æ –Ω–µ —Å–º–æ–≥ –µ—ë –∑–∞–ª–∏—Ç—å –≤ —Ä–µ–ø—É (—Å–º–æ–≥, –Ω–æ –æ–Ω–∞ –∑–∞—Ç–µ—Ä–ª–∞ –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å, —Ç–∞–∫ —á—Ç–æ —è —Ä–µ—à–∏–ª –æ–±—Ä–∞—Ç–Ω–æ –æ–±—É—á–∏—Ç—å \"—É–º–Ω—É—é\" –±–µ–π–∑–ª–∞–π–Ω–æ–≤—É—é, –∫–æ—Ç–æ—Ä–∞—è —Å–ø—Ä–∞–≤–ª—è–ª–∞—Å—å —Å –∑–∞–¥–∞—á–µ–π –ª—É—à—á–µ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —Å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ RoPE –∏ MHLA\n",
        "\n",
        "#config = model_configs[\"nano\"]\n",
        "#model = ModifiedTransformerForCausalLM(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMUsjHl4Nkoa",
        "outputId": "705ff33c-a038-438c-f2a9-e6e65bd504ab"
      },
      "outputs": [],
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–µ—Ä–∞\n",
        "\n",
        "trainer = Trainer(learning_rate=3e-4) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "id": "1nUgUfpKK993",
        "outputId": "845701be-08b3-4835-c407-e2a7195c35e7"
      },
      "outputs": [],
      "source": [
        "# –û–±—É—á–µ–Ω–∏–µ goes brrrr!\n",
        "\n",
        "trainer.run(model, train_dataloader, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"–®—Ç–∏—Ä–ª–∏—Ü –ø—Ä–∏—à–µ–ª –¥–æ–º–æ–π\"\n",
        "input_ids = torch.tensor(tokenizer.encode(text)[:-1], device=trainer.device)[None, :]\n",
        "print(input_ids)\n",
        "model_output = model.generate(\n",
        "    input_ids, max_new_tokens=200, eos_token_id=tokenizer.eos_token_id, temperature=1.6, do_sample=True, top_k=10\n",
        ")\n",
        "tokenizer.decode(model_output[0].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "YFi7M9ExHWv9",
        "outputId": "5d4d7528-7475-4f28-8543-c35607c03430"
      },
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Ö–∞–±\n",
        "\n",
        "model.push_to_hub(REPO_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WrMwV9zK993"
      },
      "source": [
        "–ü–æ–∏–≥—Ä–∞–π—Ç–µ—Å—å —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±—É—á–∏—Ç—å `mini` –∏ `small` –≤–µ—Ä—Å–∏–∏.\n",
        "–ü–æ—Å—Ç–∞—Ä–∞–π—Ç–µ—Å—å –¥–æ–±–∏—Ç—å—Å—è –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–∞–∫ –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö –ª–æ—Å—Å–∞, —Ç–∞–∫ –∏ –ø—Ä–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\n",
        "\n",
        "### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∞–ª–ª—ã\n",
        "\n",
        "–í—ã —Ç–∞–∫–∂–µ –º–æ–∂–Ω–æ –∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∞–ª–ª—ã:\n",
        "- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å Rotary Positional Embedding **[4 –±–∞–ª–ª–∞]**\n",
        "- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å Multi-Head Latent Attention **[2 –±–∞–ª–ª]**\n",
        "- –û—Ñ–æ—Ä–º–∏—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –Ω–∞ ü§ó: –∫–∞—Ä—Ç–æ—á–∫–∞ –º–æ–¥–µ–ª–∏ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∑–∞–¥–∞–Ω–∏—è, —Ä–µ–ø–æ—Ä—Ç–æ–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ **[2 –±–∞–ª–ª]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so8bIDy5dKXM"
      },
      "source": [
        "# –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª –¥–ª—è –ø—Ä–æ–≤–µ—Ä—è—é—â–µ–≥–æ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZplshN5HtRb"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer.from_pretrained(REPO_NAME)\n",
        "check_model = TransformerForCausalLM.from_pretrained(REPO_NAME)\n",
        "check_model = check_model.to(device)\n",
        "check_model = check_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "araF_3noK994"
      },
      "outputs": [],
      "source": [
        "text = \"–®—Ç–∏—Ä–ª–∏—Ü –ø—Ä–∏—à–µ–ª –¥–æ–º–æ–π\"\n",
        "input_ids = torch.tensor(tokenizer.encode(text), device=device)\n",
        "\n",
        "model_output = check_model.generate(\n",
        "    input_ids[None, :], max_new_tokens=200, eos_token_id=tokenizer.eos_token_id, temperature=2.0, do_sample=True, top_k=10\n",
        ")\n",
        "tokenizer.decode(model_output[0].tolist())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp2_env",
      "language": "python",
      "name": "nlp2_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "013f1b28161c4b2888981b2279309a08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07238cfa6bd34bf0917a8622a1cd3b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e3f8d3e5e5b4492a59f0d7d3186877e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1208ee3c939b4e18bcbf992a00bde69c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_513fd997a1444193bcaa161bb7c4e6b7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_963ec243a52f481d8e67977600e9b34d",
            "value": "‚Äá135497/135497‚Äá[00:09&lt;00:00,‚Äá15155.09it/s]"
          }
        },
        "17262cca283248a98297387d6824d5bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed5bfb7134314419a909b305e43d432b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f0ae019189404e27a4286b05922c8f9e",
            "value": "Loading‚Äádata:‚Äá100%"
          }
        },
        "1a8fccc151714a19856de84a0416d3d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e923b43f03f4bfe8c1315d8e98e049b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f3b2354ef974753bf0057ff07cdd208": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23829bfc269042948448534c24709134": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afb8e5c8b9ef4336bf6e2eee88d4e114",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ecd900fa3e114439822257ba77381f0a",
            "value": "README.md:‚Äá100%"
          }
        },
        "24a0f86cb4504f6a88c4bcc9410a6882": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25279ff26e0046899b173f33e3cea185": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27d8bbba523844ce9568c9d8b6690cfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4cf7e2c4a664fa398e6e60ae11e2348",
              "IPY_MODEL_808c00da14224708918d15a5000434fc",
              "IPY_MODEL_da58846238f54b6790acd3898da7b9a6"
            ],
            "layout": "IPY_MODEL_880b38716d5f4eb5b4f7dfba3390dc45"
          }
        },
        "34bf217aa66f4fb29d09f35f592c1bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e38bd08f29d40559bc7d339e03bd4c2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8f3dde7227494dd9b0774ce28b388274",
            "value": "dataset.csv:‚Äá100%"
          }
        },
        "408079bc18cf4307b7a4aa0a81591979": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "466b7df8811a413c95f08a580de6b89a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "468d88309904488a817e7b81a392c8b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "494d0853e39641daa0805645ce6beb17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34bf217aa66f4fb29d09f35f592c1bc9",
              "IPY_MODEL_d49f51a25b964249852b50a2acc49634",
              "IPY_MODEL_d6730505ea7d431fbfd80aa7a01dc790"
            ],
            "layout": "IPY_MODEL_73a9344dc79e43f091c8d1b223cbcee4"
          }
        },
        "4b35e808896a4a37aab51a068f242c43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bc0fba67ddf478991c47aa6cab98ff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6c63b0273ea460eb5e153e48101e332",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07238cfa6bd34bf0917a8622a1cd3b0d",
            "value": 1
          }
        },
        "4bcfbe536a2d47a39d4293d998636243": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c1ebcad0d764e60a4948434d9993a2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e38bd08f29d40559bc7d339e03bd4c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "513fd997a1444193bcaa161bb7c4e6b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51b3b59a23ad411988fc656d30d8fc8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a8fccc151714a19856de84a0416d3d8",
            "max": 135497,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e923b43f03f4bfe8c1315d8e98e049b",
            "value": 135497
          }
        },
        "51fa888c7bf34a74a0742fc197d9487a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "564c90bbde8442f59aa264ace4f8a90b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "566df9d46f7943b7b672fe7dd9f7417c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d1434a6307d4ba9bed46f5d4ae730c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_408079bc18cf4307b7a4aa0a81591979",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c541c5f56dd2474b84182f9703b4cda7",
            "value": "‚Äá150553/0‚Äá[00:02&lt;00:00,‚Äá85464.42‚Äáexamples/s]"
          }
        },
        "69fa3278c9ff402784c86c44d74e3e70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cb7655e3cd541e5855462433aa534be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7007abc4f18e4d7997aa1d7265c25eb4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73a9344dc79e43f091c8d1b223cbcee4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74a8ce73ceb64b688147bb7a71d4c2c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75dbd3da373541b2bc952f7f6a164624": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75ddc70c138c42389c14a293a68aade2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "808c00da14224708918d15a5000434fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86d07832c915484fa50db62c333922ec",
            "max": 1024,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51fa888c7bf34a74a0742fc197d9487a",
            "value": 1024
          }
        },
        "8142e932c2b94414bd27d4223c3a3d6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82a2000df7e543029f0964f399642d5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cb7655e3cd541e5855462433aa534be",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_74a8ce73ceb64b688147bb7a71d4c2c9",
            "value": "‚Äá99.0/99.0‚Äá[00:00&lt;00:00,‚Äá6.21kB/s]"
          }
        },
        "86d07832c915484fa50db62c333922ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "880b38716d5f4eb5b4f7dfba3390dc45": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f3dde7227494dd9b0774ce28b388274": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91e13de6ce154bb09173c4f9f333611b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4f01f1d458a453d8241de282a374b19",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b26a32b517324844b77e3de793b6c47d",
            "value": "Fetching‚Äá3‚Äáfiles:‚Äá100%"
          }
        },
        "946f2a1b3aa34be2a278e291fdb530e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "963ec243a52f481d8e67977600e9b34d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9970c0d3b42b4ff08d31eeae1d0efe51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fdad1a5749346b09daf54d983a22964": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a09b3fed74ed42efb5a3f71e5eae676c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17262cca283248a98297387d6824d5bb",
              "IPY_MODEL_51b3b59a23ad411988fc656d30d8fc8a",
              "IPY_MODEL_1208ee3c939b4e18bcbf992a00bde69c"
            ],
            "layout": "IPY_MODEL_4c1ebcad0d764e60a4948434d9993a2f"
          }
        },
        "a4f01f1d458a453d8241de282a374b19": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7cfb7fe4e0e484784c16947848f296e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23829bfc269042948448534c24709134",
              "IPY_MODEL_c59603b6d2484a95b18f009a81d5bc66",
              "IPY_MODEL_82a2000df7e543029f0964f399642d5d"
            ],
            "layout": "IPY_MODEL_f4de1a432cd54914939b6b70e6af7dcc"
          }
        },
        "afb8e5c8b9ef4336bf6e2eee88d4e114": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b08ba319d12f4d15bd7ec8bea32a90f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b26a32b517324844b77e3de793b6c47d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4cf7e2c4a664fa398e6e60ae11e2348": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fdad1a5749346b09daf54d983a22964",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9970c0d3b42b4ff08d31eeae1d0efe51",
            "value": "Building‚Äávocabulary:‚Äá100%"
          }
        },
        "b976e96a647b4f40a50965b3cef1c8a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9b9619cfb0f4451985c6466b0486fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd8e490fbdc5420f97d4c94f5e61b5c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25279ff26e0046899b173f33e3cea185",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_24a0f86cb4504f6a88c4bcc9410a6882",
            "value": "Generating‚Äátrain‚Äásplit:‚Äá"
          }
        },
        "be6ceb6f27af40aba535c445806e297b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75ddc70c138c42389c14a293a68aade2",
            "max": 15056,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_946f2a1b3aa34be2a278e291fdb530e8",
            "value": 15056
          }
        },
        "c541c5f56dd2474b84182f9703b4cda7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c59603b6d2484a95b18f009a81d5bc66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6975b8856484e2f96d78fe4a23a8e10",
            "max": 99,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4bcfbe536a2d47a39d4293d998636243",
            "value": 99
          }
        },
        "ce6945736f7c4c839190e0404f370a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91e13de6ce154bb09173c4f9f333611b",
              "IPY_MODEL_f81001a64a784ae7a8d09fe258376e1d",
              "IPY_MODEL_ffa04c096fc0464db8182467a307a2ae"
            ],
            "layout": "IPY_MODEL_b976e96a647b4f40a50965b3cef1c8a5"
          }
        },
        "d49f51a25b964249852b50a2acc49634": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e3f8d3e5e5b4492a59f0d7d3186877e",
            "max": 42450463,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_566df9d46f7943b7b672fe7dd9f7417c",
            "value": 42450463
          }
        },
        "d6730505ea7d431fbfd80aa7a01dc790": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_466b7df8811a413c95f08a580de6b89a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_75dbd3da373541b2bc952f7f6a164624",
            "value": "‚Äá42.5M/42.5M‚Äá[00:00&lt;00:00,‚Äá156MB/s]"
          }
        },
        "d6c63b0273ea460eb5e153e48101e332": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "da58846238f54b6790acd3898da7b9a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7007abc4f18e4d7997aa1d7265c25eb4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8142e932c2b94414bd27d4223c3a3d6c",
            "value": "‚Äá1024/1024‚Äá[17:39&lt;00:00,‚Äá‚Äá1.35s/it]"
          }
        },
        "e4231b41f41d421facafcdefb89969d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd8e490fbdc5420f97d4c94f5e61b5c5",
              "IPY_MODEL_4bc0fba67ddf478991c47aa6cab98ff4",
              "IPY_MODEL_5d1434a6307d4ba9bed46f5d4ae730c0"
            ],
            "layout": "IPY_MODEL_f470bf33537d462a883ef2d57df8bfbc"
          }
        },
        "e5087d27c4f14a28bfb73740fa147d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef5bee8e8d2f4d8c91ba14e77dda8d01",
              "IPY_MODEL_be6ceb6f27af40aba535c445806e297b",
              "IPY_MODEL_f2a0e82016804d2ca04aeb19c0d936e5"
            ],
            "layout": "IPY_MODEL_564c90bbde8442f59aa264ace4f8a90b"
          }
        },
        "e6975b8856484e2f96d78fe4a23a8e10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecd900fa3e114439822257ba77381f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed5bfb7134314419a909b305e43d432b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef5bee8e8d2f4d8c91ba14e77dda8d01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f3b2354ef974753bf0057ff07cdd208",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_468d88309904488a817e7b81a392c8b3",
            "value": "100%"
          }
        },
        "f0ae019189404e27a4286b05922c8f9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2a0e82016804d2ca04aeb19c0d936e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f94a9f0d86294b0288ad87412ec2ca5d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_69fa3278c9ff402784c86c44d74e3e70",
            "value": "‚Äá15056/15056‚Äá[00:14&lt;00:00,‚Äá853.37it/s]"
          }
        },
        "f470bf33537d462a883ef2d57df8bfbc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4de1a432cd54914939b6b70e6af7dcc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f81001a64a784ae7a8d09fe258376e1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_013f1b28161c4b2888981b2279309a08",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9b9619cfb0f4451985c6466b0486fb9",
            "value": 3
          }
        },
        "f94a9f0d86294b0288ad87412ec2ca5d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffa04c096fc0464db8182467a307a2ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b35e808896a4a37aab51a068f242c43",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b08ba319d12f4d15bd7ec8bea32a90f4",
            "value": "‚Äá3/3‚Äá[00:00&lt;00:00,‚Äá202.59it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
